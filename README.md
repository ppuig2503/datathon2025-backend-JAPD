# Datathon 2025 Backend - JAPD

Backend API para predicciÃ³n y explicabilidad de oportunidades de venta usando Machine Learning (LightGBM).

## ğŸ“‹ Tabla de Contenidos

- [DescripciÃ³n](#descripciÃ³n)
- [Requisitos Previos](#requisitos-previos)
- [InstalaciÃ³n](#instalaciÃ³n)
- [EjecuciÃ³n](#ejecuciÃ³n)
  - [Modo Desarrollo (Swagger)](#modo-desarrollo-swagger)
  - [ConexiÃ³n con Frontend](#conexiÃ³n-con-frontend)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Endpoints Disponibles](#endpoints-disponibles)
- [Flujo de Uso](#flujo-de-uso)

## ğŸ“ DescripciÃ³n

Esta API REST proporciona:
- **PredicciÃ³n**: PredicciÃ³n binaria (GANADA/PERDIDA) de oportunidades de venta
- **Explicabilidad Local**: LIME y SHAP local para explicar predicciones individuales
- **Explicabilidad Global**: SHAP global y Partial Dependence Plots (PDP)
- **GeneraciÃ³n de Textos**: Explicaciones en lenguaje natural usando OpenAI GPT-4

## ğŸ”§ Requisitos Previos

- **Python 3.8+**
- **pip** (gestor de paquetes de Python)
- **API Key de OpenAI** (para endpoints de explicabilidad con IA)

## ğŸ“¦ InstalaciÃ³n

### 1. Clonar el repositorio
```bash
git clone <repository-url>
cd datathon2025-backend-JAPD
```

### 2. Instalar dependencias

Ejecuta el script de instalaciÃ³n de dependencias:

**Linux / Mac / Git Bash:**
```bash
bash install_deps.sh
```

**Windows (PowerShell):**
```powershell
bash install_deps.sh
```

**Windows (cmd) - instalaciÃ³n manual:**
```cmd
pip install fastapi uvicorn pydantic pandas joblib scikit-learn lightgbm openai httpx python-dotenv scikit-image lime shap pdpbox
```

### 3. Configurar variables de entorno

Crea un archivo `.env` en la raÃ­z del proyecto:

```env
OPENAI_API_KEY=tu_clave_de_openai_aqui
```

> **Nota**: Solicita la API key de OpenAI al equipo o crea una en [platform.openai.com](https://platform.openai.com)

## ğŸš€ EjecuciÃ³n

### Modo Desarrollo (Swagger)

Para probar los endpoints directamente desde la interfaz Swagger:

**Windows (cmd):**
```cmd
python main.py
```

**Windows (PowerShell) / Linux / Mac:**
```bash
python main.py
```

La API estarÃ¡ disponible en:
- **Servidor local**: `http://localhost:8000`
- **DocumentaciÃ³n Swagger**: `http://localhost:8000/docs`
- **DocumentaciÃ³n alternativa (ReDoc)**: `http://localhost:8000/redoc`

### ConexiÃ³n con Frontend

#### ConfiguraciÃ³n CORS

El backend ya estÃ¡ configurado para aceptar peticiones desde el frontend. Por defecto permite conexiones desde:
- `http://localhost:3000` (React/Next.js por defecto)
- `http://localhost:5173` (Vite por defecto)
- `http://localhost:4200` (Angular por defecto)

Si tu frontend corre en otro puerto, edita `main.py`:

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "http://localhost:5173",
        "http://localhost:4200",
        "http://localhost:TU_PUERTO_AQUI"  # AÃ±adir tu puerto
    ],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

## ğŸ“ Estructura del Proyecto

```
datathon2025-backend-JAPD/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ controlers/
â”‚   â”‚   â”œâ”€â”€ mlControler.py              # Endpoint de predicciÃ³n
â”‚   â”‚   â”œâ”€â”€ openAiControler.py          # Endpoints con OpenAI
â”‚   â”‚   â””â”€â”€ explainability/
â”‚   â”‚       â””â”€â”€ lightGBM/
â”‚   â”‚           â”œâ”€â”€ limeControler.py    # Explicabilidad LIME
â”‚   â”‚           â”œâ”€â”€ shapControler.py    # Explicabilidad SHAP
â”‚   â”‚           â””â”€â”€ pdpControler.py     # Partial Dependence Plots
â”‚   â””â”€â”€ types/
â”‚       â”œâ”€â”€ mlTypes.py                  # Modelos Pydantic para ML
â”‚       â””â”€â”€ openAiTypes.py              # Modelos Pydantic para OpenAI
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ data.py                         # Almacenamiento en memoria
â”‚   â””â”€â”€ dataset.csv                     # Dataset de entrenamiento
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lgbm/
â”‚       â”œâ”€â”€ lgbm_classifier.joblib      # Modelo entrenado
â”‚       â”œâ”€â”€ X_train_sample.joblib       # Muestra de entrenamiento
â”‚       â””â”€â”€ X_test.joblib               # Datos de test
â”œâ”€â”€ main.py                             # Punto de entrada de la aplicaciÃ³n
â”œâ”€â”€ install_deps.sh                     # Script de instalaciÃ³n de dependencias
â”œâ”€â”€ .env                                # Variables de entorno (crear)
â””â”€â”€ README.md                           # Este archivo
```

## ğŸ”Œ Endpoints Disponibles

### Machine Learning (`/ml`)

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| POST | `/ml/predict` | Realizar predicciÃ³n (devuelve 0 o 1) |
| POST | `/ml/explain_lime` | ExplicaciÃ³n LIME local |
| POST | `/ml/explain_shap_local` | ExplicaciÃ³n SHAP local |
| POST | `/ml/explain_shap_global` | Importancias SHAP globales |
| POST | `/ml/explain_pdp` | Partial Dependence Plot |

### OpenAI (`/ai`)

| MÃ©todo | Endpoint | DescripciÃ³n |
|--------|----------|-------------|
| POST | `/ai/welcome` | Mensaje de bienvenida explicando LGBM |
| POST | `/ai/text/local` | ExplicaciÃ³n textual de predicciÃ³n local |
| POST | `/ai/text/global` | ExplicaciÃ³n textual de patrones globales |
| POST | `/ai/pdp_sentence` | Resumen de grÃ¡fico PDP en una frase |
| POST | `/ai/answer` | Chatbot para responder dudas |

## ğŸ”„ Flujo de Uso

### 1. Realizar PredicciÃ³n
```
POST /ml/predict
```
Guarda automÃ¡ticamente los datos de entrada y predicciÃ³n en memoria.

### 2. Obtener Explicabilidad Local
```
POST /ml/explain_lime
O
POST /ml/explain_shap_local
```
Usa los datos guardados del paso 1. No requiere parÃ¡metros.

### 3. Obtener Explicabilidad Global
```
POST /ml/explain_shap_global
O
POST /ml/explain_pdp?feature_to_analyze=nombre_feature
```

### 4. Generar Textos Explicativos
```
POST /ai/welcome          # Bienvenida inicial
POST /ai/text/local       # ExplicaciÃ³n de predicciÃ³n individual
POST /ai/text/global      # ExplicaciÃ³n de patrones del modelo
POST /ai/pdp_sentence     # Resumen de PDP
```

### 5. Chatbot Interactivo
```
POST /ai/answer
Body: { "question": "Â¿Por quÃ© se predijo ganada?" }
```

## ğŸ› SoluciÃ³n de Problemas

### Error: "Model not loaded"
- Verifica que existe `models/lgbm/lgbm_classifier.joblib`
- AsegÃºrate de haber entrenado y guardado el modelo correctamente

### Error: "No prediction data available"
- Debes llamar primero a `/ml/predict` antes de los endpoints de explicabilidad

### Error: "OpenAI API error"
- Verifica que tu `OPENAI_API_KEY` en `.env` es correcta
- Confirma que tienes crÃ©ditos disponibles en tu cuenta de OpenAI

### CORS error desde el frontend
- AÃ±ade el puerto de tu frontend en la configuraciÃ³n CORS de `main.py`
- Verifica que el frontend estÃ¡ usando la URL correcta: `http://localhost:8000`

## ğŸ“Š Ejemplo Completo (usando curl)

```bash
# 1. Realizar predicciÃ³n
curl -X POST "http://localhost:8000/ml/predict" \
  -H "Content-Type: application/json" \
  -d '{"feature1": 1.5, "feature2": 0.8, ...}'

# 2. Obtener explicaciÃ³n LIME
curl -X POST "http://localhost:8000/ml/explain_lime"

# 3. Obtener texto explicativo
curl -X POST "http://localhost:8000/ai/text/local"

# 4. Hacer pregunta al chatbot
curl -X POST "http://localhost:8000/ai/answer" \
  -H "Content-Type: application/json" \
  -d '{"question": "Â¿QuÃ© variables son mÃ¡s importantes?"}'
```

## ğŸ‘¥ Equipo

Proyecto desarrollado por el equipo JAPD para el Datathon 2025.